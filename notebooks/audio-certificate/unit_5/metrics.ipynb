{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc44bb72",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e7754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243f37f",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef33b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"the cat sat on the mat\"\n",
    "prediction = \"the cat sit on the\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a203f",
   "metadata": {},
   "source": [
    "### WER (Word Error Rate)\n",
    "\n",
    "The word error rate (WER) metric is the ‘de facto’ metric for speech recognition. It calculates substitutions, insertions and deletions on the word level. This means errors are annotated on a word-by-word basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5853e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "wer_metric = load(\"wer\")\n",
    "wer = wer_metric.compute(references=[reference], predictions=[prediction])\n",
    "\n",
    "print(wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75c197",
   "metadata": {},
   "source": [
    "### Word Accuracy\n",
    "\n",
    "Word Accuracy = 1 - WER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daad575b",
   "metadata": {},
   "source": [
    "### Character Error Rate (CER)\n",
    "\n",
    "It seems a bit unfair that we marked the entire word for “sit” wrong when in fact only one letter was incorrect. That’s because we were evaluating our system on the word level, thereby annotating errors on a word-by-word basis. The character error rate (CER) assesses systems on the character level. This means we divide up our words into their individual characters, and annotate errors on a character-by-character basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d877f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22727272727272727\n"
     ]
    }
   ],
   "source": [
    "cer_metric = load(\"cer\")\n",
    "cer = cer_metric.compute(references=[reference], predictions=[prediction])\n",
    "\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c96e52",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "If we train an ASR model on data with punctuation and casing, it will learn to predict casing and punctuation in its transcriptions. This is great when we want to use our model for actual speech recognition applications, such as transcribing meetings or dictation, since the predicted transcriptions will be fully formatted with casing and punctuation, a style referred to as orthographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1ce9c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' he tells us that at this festive season of the year with christmas and roast beef looming before us similarly is drawn from eating and its results occur most readily to the mind '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "prediction = \" He tells us that at this festive season of the year, with Christmas and roast beef looming before us, similarly is drawn from eating and its results occur most readily to the mind.\"\n",
    "normalized_prediction = normalizer(prediction)\n",
    "\n",
    "normalized_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77ef0f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"HE TELLS US THAT AT THIS FESTIVE SEASON OF THE YEAR WITH CHRISTMAS AND ROAST BEEF LOOMING BEFORE US SIMILES DRAWN FROM EATING AND ITS RESULTS OCCUR MOST READILY TO THE MIND\"\n",
    "normalized_referece = normalizer(reference)\n",
    "\n",
    "wer = wer_metric.compute(\n",
    "    references=[normalized_referece], predictions=[normalized_prediction]\n",
    ")\n",
    "wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdfb8d6",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f6ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    torch_dtype = torch.float16\n",
    "    \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    torch_dtype = torch.float32\n",
    "\n",
    "    \n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "209b6829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9dbd6c7ce1b4719a19c25ae20b1f9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7c7ac5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6035665077e242f6894cbe5f27b8c800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/152M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d6a47e0ba94cd59ae39944e6c08e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/129M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730be995d1c440cd8c34b11f86bcb5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/137M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7390c99b1e44529eac46ec9f9f770a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b7488065f0473da75eb873e6b638bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/344M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ee991a3bf24754b972daf82da03f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/882k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187ac8aad8574db5b791dd4e991d6dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/750k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6fc3c46df94356ace686244093a758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/713k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a2ee91fb6040e1a133c133712bcf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/150k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd0cfa21487405193ad592947330423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee4f5db2f16a45f69ad399b79cc04d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 3138it [00:00, 175041.57it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e6c9e7d82045fdb4b2efaee59ec780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 2638it [00:00, 242107.92it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c04ef5a48748418f1d94e47dc8d23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 2638it [00:00, 252517.83it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe9709b9346a444897238f5ed4a467db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating other split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 506it [00:00, 221559.43it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027f5ffb4fa14b5e8a621ae25c559ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating invalidated split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading metadata...: 6697it [00:00, 230264.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "common_voice_test = load_dataset(\n",
    "    \"mozilla-foundation/common_voice_13_0\", \"et\", split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0572b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2638/2638 [34:52<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "# run streamed inference\n",
    "for prediction in tqdm(\n",
    "    pipe(\n",
    "        KeyDataset(common_voice_test, \"audio\"),\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\"task\": \"transcribe\", 'language': 'et'},\n",
    "        batch_size=32,\n",
    "    ),\n",
    "    total=len(common_voice_test),\n",
    "):\n",
    "    all_predictions.append(prediction[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b12f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.25920197958553"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    "\n",
    "wer_ortho = 100 * wer_metric.compute(\n",
    "    references=common_voice_test[\"sentence\"], predictions=all_predictions\n",
    ")\n",
    "wer_ortho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59f6e130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.7370515470278"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "# compute normalised WER\n",
    "all_predictions_norm = [normalizer(pred) for pred in all_predictions]\n",
    "all_references_norm = [normalizer(label) for label in common_voice_test[\"sentence\"]]\n",
    "\n",
    "# filtering step to only evaluate the samples that correspond to non-zero references\n",
    "all_predictions_norm = [\n",
    "    all_predictions_norm[i]\n",
    "    for i in range(len(all_predictions_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "all_references_norm = [\n",
    "    all_references_norm[i]\n",
    "    for i in range(len(all_references_norm))\n",
    "    if len(all_references_norm[i]) > 0\n",
    "]\n",
    "\n",
    "wer = 100 * wer_metric.compute(\n",
    "    references=all_references_norm, predictions=all_predictions_norm\n",
    ")\n",
    "\n",
    "wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ffe5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging_face",
   "language": "python",
   "name": "hugging_face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
